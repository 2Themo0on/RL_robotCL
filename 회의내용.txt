1. 로봇 청소기의 탐색 기능을 구현하기 위해서 우리는 환경을 알고 있지만
nbym map agent가 모른다는 상황을 가정하고
모든 map 좌표를 탐색하면 이동 불/가 map을 그린다

2. 그린 map을 가지고 학습 단계에서 활용을 하게 된다
4개의 이동 func을 선택할 때 map matrix에서 가능여부에 따라 q-value를 같이 고려한다.

3. 우선 Q-Learning 모델 완성하고 Hyper Param 평가해본뒤 
그 후에 다른 모델을 만들어서 비교해보는 흐름으로 가져가자.

4. 장애물은 랜덤으로 만든 방 하나, 방2 같은 식으로 만들자.

5. Step 진행 시마다 Q - Value는 갱신된다 

===================================================================

5.25
1. 결국 grid의 모든 좌표를 청소해야 로봇 청소기의 목적에 부합한다.
2. 보상을 어떻게 정의할 지 고려해봐야한다
3. 전체 grid를 모두 방문하되 최소한의 시간으로 소요되는 경로를 이동하게 할 것인지
4. 최종목적지를 방문한 뒤 dock에 돌아와야 하는 특성이 있다.

우선 
→ 그렇다면 중앙을 방문하지 않고 외곽으로만 경로가 형성되지 않겠나? 
목적지를 3개에서 4개로 증가시키자 

목적지 1 max(x,y)
목적지 2 (0,max(y))
목적지 3 (max(x), 0)
목적지 4 1/2(max(x,y))

좋은데 이러면 각 목적지 4개만 방문하기 위한 최적 경로가 되어 버릴 것 같은데
grey zone이 발생하지 않겠나?

1. 총보상을 최대화 하는 방향으로 이동하고 
  모든 point에 보상을 추가하면 어떨까?

2. 4개 목적지 근처에 보상을 추가해야하나?

3. 확률적 이동 조건이 들어가야하나? 

한 구역에 진입했을때 구역을 떠나기 전에 모든 포인트를 다 방문해야하는데
어떻게 보상을 정의해야 원하는 결과가 나올지 고민해야한다.

시중에 나와있는 로봇청소기들이 목적지를 정하고 방문하는 방식은?

→ 실제 로봇  청소기는 시간에 제약이 적지 않나?
 미방문한 point가 없는 것이 더 중요하다고 생각


====================================================
Q-learning Epsilon 이런식으로 감소하게 
# Exploration parameters
epsilon = 1.0                 # Exploration rate
max_epsilon = 1.0             # Exploration probability at start
min_epsilon = 0.01            # Minimum exploration probability 
decay_rate = 0.005            # Exponential decay rate for exploration prob

=======================================================
Grid를 만들 때 벽을 어떻게 구현할 건지?
생성한 그리드 외곽을 2로 할당해서 
agent가 탐색할 때 2를 만나면 방향 전환하도록 
